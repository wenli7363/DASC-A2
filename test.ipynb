{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dd797e6",
   "metadata": {},
   "source": [
    "# 1 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b93988-4bb4-48de-822b-671c497b4be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import build_dataset,preprocess_data\n",
    "from utils import not_change_test_dataset\n",
    "from tokenizer import initialize_tokenizer\n",
    "from datasets import Dataset, DatasetDict, IterableDataset, IterableDatasetDict, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ece0bbb-e52b-4d1b-b3b5-f27dd1c38d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for the test dataset, don't change these\n",
    "TEST_DATASET_ROW_NUMBER = 471911  # Number of rows of the test dataset\n",
    "TEST_DATASET_SIZE_IN_BYTES = 205026320  # Size of the test dataset in bytes\n",
    "TEST_DATASET_FINGERPRINT = \"e1bc80da8b43b9f4\"  # Fingerprint of the test dataset\n",
    "\n",
    "def not_change_test_dataset(raw_datasets: DatasetDict) -> bool:\n",
    "    \"\"\"\n",
    "    Check if the test dataset is not changed.\n",
    "\n",
    "    Args:\n",
    "        raw_datasets: Raw datasets.\n",
    "\n",
    "    Returns:\n",
    "        True if the test dataset is not changed, False otherwise.\n",
    "    \"\"\"\n",
    "    raw_datasets_test = raw_datasets[\"test\"]\n",
    "    return (\n",
    "        raw_datasets_test.num_rows == TEST_DATASET_ROW_NUMBER\n",
    "        and raw_datasets_test.size_in_bytes == TEST_DATASET_SIZE_IN_BYTES\n",
    "        and raw_datasets_test._fingerprint == TEST_DATASET_FINGERPRINT\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bc29cf-aec6-47c6-8d41-7afad0c2dd6c",
   "metadata": {},
   "source": [
    "# 1 导入数据集，验证数据集的正确性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21d9d5b7-11ae-429d-bf27-a26279b07e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since tomaarsen/MultiCoNER couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'multi' at /root/.cache/huggingface/datasets/tomaarsen___multi_co_ner/multi/1.0.0/04fafbabb7a36defcfd400ed2fe5504c10fdf4ab (last modified on Sat Apr 12 17:31:08 2025).\n",
      "Using the latest cached version of the dataset since tomaarsen/MultiCoNER couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'multi' at /root/.cache/huggingface/datasets/tomaarsen___multi_co_ner/multi/1.0.0/04fafbabb7a36defcfd400ed2fe5504c10fdf4ab (last modified on Sat Apr 12 17:31:08 2025).\n",
      "Using the latest cached version of the dataset since tomaarsen/MultiCoNER couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'multi' at /root/.cache/huggingface/datasets/tomaarsen___multi_co_ner/multi/1.0.0/04fafbabb7a36defcfd400ed2fe5504c10fdf4ab (last modified on Sat Apr 12 17:31:08 2025).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = build_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "00bdbeda-f3ea-4160-840b-e780dfd00afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 168300\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 8800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'ner_tags'],\n",
      "        num_rows: 471911\n",
      "    })\n",
      "})\n",
      "============================================================\n",
      "Dataset({\n",
      "    features: ['id', 'tokens', 'ner_tags'],\n",
      "    num_rows: 168300\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets)\n",
    "print(\"==\"*30)\n",
    "print(raw_datasets['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9ff4282-cc55-48bc-9914-0e1e367b4b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 0, 'tokens': ['his', 'playlist', 'includes', 'sonny', 'sharrock', ',', 'gza', ',', 'country', 'teasers', 'and', 'the', 'notorious', 'b.i.g.'], 'ner_tags': [0, 0, 0, 1, 2, 0, 1, 0, 7, 8, 0, 1, 2, 2]}\n"
     ]
    }
   ],
   "source": [
    "print(raw_datasets['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de80cc52-c0c2-42ea-a74d-d1d68165857a",
   "metadata": {},
   "source": [
    "# 2 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c63c28f3-1b75-49f5-8bee-87303ba79e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import initialize_tokenizer\n",
    "\n",
    "tokenizer = initialize_tokenizer()\n",
    "tokenized_datasets = preprocess_data(raw_datasets, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3174274b-07e4-4afa-a489-24b71dc268d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 168300\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 8800\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 471911\n",
      "    })\n",
      "})\n",
      "\n",
      "============================================================\n",
      "\n",
      "Dataset({\n",
      "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 168300\n",
      "})\n",
      "\n",
      "============================================================\n",
      "\n",
      "{'input_ids': [101, 1117, 1505, 7276, 2075, 1488, 3382, 188, 7111, 10411, 117, 176, 3293, 117, 1583, 20826, 1733, 1105, 1103, 14140, 171, 119, 178, 119, 176, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [-100, 0, 0, 1, 0, 1, 2, 2, 3, 3, 0, 1, 2, 0, 7, 8, 9, 0, 1, 2, 2, 3, 3, 3, 3, 3, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_datasets)\n",
    "\n",
    "print()\n",
    "print(\"==\" * 30)\n",
    "print()\n",
    "\n",
    "print(tokenized_datasets[\"train\"])\n",
    "\n",
    "print()\n",
    "print(\"==\" * 30)\n",
    "print()\n",
    "\n",
    "print(tokenized_datasets[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6be626d-3423-48fa-bba0-9bd3d855e90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "23\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_datasets[\"train\"]['input_ids'][0]))\n",
    "print(len(tokenized_datasets[\"train\"]['input_ids'][1]))\n",
    "print(len(tokenized_datasets[\"train\"]['input_ids'][20]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
